# SE363 â€“ PhÃ¡t triá»ƒn á»©ng dá»¥ng trÃªn ná»n táº£ng dá»¯ liá»‡u lá»›n
# Khoa CÃ´ng nghá»‡ Pháº§n má»m â€“ TrÆ°á»ng Äáº¡i há»c CÃ´ng nghá»‡ ThÃ´ng tin, ÄHQG-HCM
# HopDT â€“ Faculty of Software Engineering, University of Information Technology (FSE-UIT)

# consumer_postgres_streaming.py
# ======================================
# Consumer Ä‘á»c dá»¯ liá»‡u tá»« Kafka topic "absa-reviews"
# â†’ cháº¡y inference mÃ´ hÃ¬nh ABSA (.pt)
# â†’ ghi káº¿t quáº£ vÃ o PostgreSQL
# â†’ Airflow sáº½ giÃ¡m sÃ¡t vÃ  khá»Ÿi Ä‘á»™ng láº¡i khi job bá»‹ dá»«ng.

from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.functions import from_json, col
import psycopg2
from psycopg2.extras import execute_values
import pandas as pd, torch, torch.nn as nn, torch.nn.functional as tF
from transformers import AutoTokenizer, AutoModel
import random, time, os, sys, json

# === 1. Spark session vá»›i Kafka connector ===
scala_version = "2.12"
spark_version = "3.5.1"

spark = (
    SparkSession.builder
    .appName("Kafka_ABSA_Postgres")
    .config(
        "spark.jars.packages",
        f"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version},"
        f"org.postgresql:postgresql:42.6.0,"
        f"org.apache.kafka:kafka-clients:3.5.1"
    )
    .config("spark.executor.instances", "1")  # Giá»›i háº¡n 1 executor
    .config("spark.executor.cores", "1")      # Giá»›i háº¡n 1 core
    .config("spark.driver.maxResultSize", "4g")  # Giá»›i háº¡n káº¿t quáº£
    .config("spark.sql.streaming.checkpointLocation", "/opt/airflow/checkpoints/absa_streaming_checkpoint")
    .config("spark.sql.execution.arrow.pyspark.enabled", "false")  # KhÃ´ng yÃªu cáº§u pyarrow
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# === 2. Äá»c dá»¯ liá»‡u streaming tá»« Kafka ===
df_stream = (
    spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "kafka-airflow:9092")
    .option("subscribe", "absa-reviews")
    .option("startingOffsets", "earliest")  # Äá»c tá»« Ä‘áº§u topic náº¿u chÆ°a cÃ³ checkpoint
    .option("failOnDataLoss", "false")
    .option("maxOffsetsPerTrigger", 5)  # Batch nhá» hÆ¡n Ä‘á»ƒ trÃ¡nh OOM
    .load()
)

df_text = df_stream.selectExpr("CAST(value AS STRING) as Review")

# === 3. Äá»‹nh nghÄ©a mÃ´ hÃ¬nh ABSA ===
ASPECTS = ["price","shipping","outlook","quality","size","shop_service","general","others"]
MODEL_NAME = "xlm-roberta-base"
TOKENIZER_LOCAL_DIR = "/opt/airflow/models/hf-cache/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089"
BACKBONE_LOCAL_DIR = TOKENIZER_LOCAL_DIR  # dÃ¹ng cÃ¹ng thÆ° má»¥c náº¿u Ä‘Ã£ Ä‘Æ°á»£c táº£i sáºµn
MODEL_PATH = "/opt/airflow/models/best_absa_hardshare.pt"
MAX_LEN = 64
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

_model, _tokenizer = None, None
_model_mtime = 0.0

class ABSAModel(nn.Module):
    def __init__(self, model_name=MODEL_NAME, num_aspects=len(ASPECTS)):
        super().__init__()
        # Æ¯u tiÃªn backbone local náº¿u cÃ³ Ä‘á»ƒ trÃ¡nh táº£i máº¡ng láº§n Ä‘áº§u
        src = BACKBONE_LOCAL_DIR if os.path.isdir(BACKBONE_LOCAL_DIR) else model_name
        try:
            if src == BACKBONE_LOCAL_DIR:
                print(f"[ABSA] Loading backbone locally from {BACKBONE_LOCAL_DIR}", flush=True)
                self.backbone = AutoModel.from_pretrained(src, local_files_only=True)
            else:
                print(f"[ABSA] Downloading backbone {model_name} (first run may take time)", flush=True)
                self.backbone = AutoModel.from_pretrained(src)
        except Exception as e:
            print(f"[ABSA] Failed to load backbone from '{src}': {e}", flush=True)
            raise
        H = self.backbone.config.hidden_size
        self.dropout = nn.Dropout(0.1)
        self.head_m = nn.Linear(H, num_aspects)
        self.head_s = nn.Linear(H, num_aspects * 3)
    def forward(self, input_ids, attention_mask):
        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        h_cls = self.dropout(out.last_hidden_state[:, 0, :])
        return self.head_m(h_cls), self.head_s(h_cls).view(-1, len(ASPECTS), 3)

# === Giáº£i mÃ£ Review JSON thÃ nh text tiáº¿ng Viá»‡t trÆ°á»›c khi stream ===
review_schema = T.StructType([
    T.StructField("id", T.StringType()),
    T.StructField("review", T.StringType())
])
df_final = df_text.withColumn("text", from_json(col("Review"), review_schema).getField("review")).select("text")

# === Inference on driver within foreachBatch to avoid UDF worker crashes ===
def _load_model_once():
    global _model, _tokenizer, _model_mtime

    # 1. Láº¥y thá»i gian sá»­a Ä‘á»•i hiá»‡n táº¡i cá»§a file mÃ´ hÃ¬nh
    try:
        current_mtime = os.path.getmtime(MODEL_PATH)
    except FileNotFoundError:
        print(f"[ABSA] ERROR: Model file not found at {MODEL_PATH}", flush=True)
        return

    # 2. Logic Hot-Reload: Táº£i láº§n Ä‘áº§u HOáº¶C náº¿u thá»i gian sá»­a Ä‘á»•i Ä‘Ã£ thay Ä‘á»•i
    if _model is None or current_mtime > _model_mtime:
        
        if _model is not None:
            print(f"[ABSA HOT-RELOAD] MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t (mtime: {_model_mtime} -> {current_mtime}), Táº¢I Láº I!", flush=True)
            
        print("[ABSA] Loading tokenizer/modelâ€¦", flush=True)

        if os.path.isdir(TOKENIZER_LOCAL_DIR):
            print(f"[ABSA] Using local tokenizer at {TOKENIZER_LOCAL_DIR}", flush=True)
            _tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_LOCAL_DIR, use_fast=True, local_files_only=True)
        else:
            print(f"[ABSA] Using remote tokenizer {MODEL_NAME}", flush=True)
            _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
        m = ABSAModel()
        state = torch.load(MODEL_PATH, map_location=DEVICE)
        if isinstance(state, dict) and "state_dict" in state:
            state = state["state_dict"]
        m.load_state_dict(state, strict=False)
        m.to(DEVICE).eval()
        _model = m

        # 4. Cáº­p nháº­t thá»i gian sá»­a Ä‘á»•i má»›i
        _model_mtime = current_mtime
        print("[ABSA] Tokenizer/model ready.", flush=True)
    else:
        # MÃ´ hÃ¬nh Ä‘Ã£ táº£i vÃ  chÆ°a thay Ä‘á»•i
        pass

def _infer_batch(texts):
    _load_model_once()
    SENTIMENTS = ["POS", "NEU", "NEG"]
    outputs = []
    for t in texts:
        t = t or ""
        enc = _tokenizer(t, truncation=True, padding="max_length", max_length=MAX_LEN, return_tensors="pt")
        enc = {k: v.to(DEVICE) for k, v in enc.items()}
        with torch.no_grad():
            _, logits_s = _model(enc["input_ids"], enc["attention_mask"])
            probs = tF.softmax(logits_s, dim=-1)[0].detach().cpu().numpy().tolist()
        row = {"text": t}
        for i, asp in enumerate(ASPECTS):
            idx = int(max(range(3), key=lambda j: probs[i][j]))
            row[asp] = SENTIMENTS[idx]
        outputs.append(row)
    return outputs

# === 5. Ghi káº¿t quáº£ vÃ o PostgreSQL (chuáº©n UTF-8, log Ä‘áº§y Ä‘á»§, xá»­ lÃ½ lá»—i an toÃ n) ===
# Auto-stop tracking
import threading
_last_batch_time = [None]  # Sáº½ Ä‘Æ°á»£c set láº§n Ä‘áº§u khi cÃ³ batch non-empty
_stop_event = threading.Event()
_batch_in_progress = [False]  # TrÃ¡nh stop khi Ä‘ang xá»­ lÃ½ batch

def write_to_postgres(batch_df, batch_id):
    sys.stdout.reconfigure(encoding='utf-8')
    pdf = batch_df.select("text").toPandas()
    if pdf.empty:
        print(f"[Batch {batch_id}] âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i.")
        return

    # ÄÃ¡nh dáº¥u báº¯t Ä‘áº§u batch Ä‘á»ƒ trÃ¡nh stop giá»¯a chá»«ng
    _batch_in_progress[0] = True
    try:
        # Cáº­p nháº­t thá»i gian batch cuá»‘i (chá»‰ khi cÃ³ data)
        _last_batch_time[0] = time.time()

        results = _infer_batch(pdf["text"].tolist())
        out_df = pd.DataFrame(results)

        print(f"\n[Batch {batch_id}] Nháº­n {len(out_df)} dÃ²ng, hiá»ƒn thá»‹ 5 dÃ²ng Ä‘áº§u:")
        print(json.dumps(out_df.head(5).to_dict(orient="records"), ensure_ascii=False, indent=2))

        try:
            cols = ["text"] + ASPECTS
            values = [tuple(row[c] for c in cols) for _, row in out_df.iterrows()]
            with psycopg2.connect(dbname="airflow", user="airflow", password="airflow", host="postgres", port=5432) as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        CREATE TABLE IF NOT EXISTS absa_results (
                            text TEXT,
                            price TEXT,
                            shipping TEXT,
                            outlook TEXT,
                            quality TEXT,
                            size TEXT,
                            shop_service TEXT,
                            general TEXT,
                            others TEXT
                        );
                        """
                    )
                    execute_values(
                        cur,
                        f"INSERT INTO absa_results ({', '.join(cols)}) VALUES %s",
                        values,
                    )
            print(f"[Batch {batch_id}] âœ… Ghi PostgreSQL thÃ nh cÃ´ng ({len(out_df)} dÃ²ng).")
        except Exception as e:
            print(f"[Batch {batch_id}] âš ï¸ KhÃ´ng thá»ƒ ghi vÃ o PostgreSQL: {e}")
    finally:
        # ÄÃ¡nh dáº¥u káº¿t thÃºc batch
        _batch_in_progress[0] = False

def monitor_inactivity(query_obj, timeout_seconds=120):
    """Dá»«ng streaming query náº¿u khÃ´ng cÃ³ batch má»›i trong timeout_seconds"""
    while not _stop_event.is_set():
        time.sleep(10)
        # Chá»‰ kiá»ƒm tra náº¿u Ä‘Ã£ cÃ³ batch non-empty
        if _last_batch_time[0] is not None:
            elapsed = time.time() - _last_batch_time[0]
            if elapsed > timeout_seconds:
                # TrÃ¡nh stop khi batch Ä‘ang xá»­ lÃ½ Ä‘á»ƒ khÃ´ng gÃ¢y Py4J lá»—i káº¿t thÃºc
                if not _batch_in_progress[0]:
                    print(f"\n[AUTO-STOP] KhÃ´ng cÃ³ batch má»›i trong {timeout_seconds}s â†’ Dá»«ng consumer.")
                    query_obj.stop()
                    _stop_event.set()
                    break
                else:
                    # Äá»£i thÃªm vÃ²ng sau khi batch xong
                    print(f"[AUTO-STOP] Chá» batch hiá»‡n táº¡i káº¿t thÃºc rá»“i má»›i dá»«ngâ€¦")

# === 6. Báº¯t Ä‘áº§u stream ===
query = (
    df_final.writeStream
    .foreachBatch(write_to_postgres)
    .outputMode("append")
    .trigger(processingTime="5 seconds")
    .start()
)

# Khá»Ÿi Ä‘á»™ng monitor thread Ä‘á»ƒ auto-stop sau 120s khÃ´ng cÃ³ batch má»›i
monitor_thread = threading.Thread(target=monitor_inactivity, args=(query, 120), daemon=True)
monitor_thread.start()

print("ğŸš€ Streaming job starting â€” chuáº©n bá»‹ láº¯ng nghe dá»¯ liá»‡u tá»« Kafka...")
print("[AUTO-STOP] Sáº½ tá»± Ä‘á»™ng dá»«ng sau 120s khÃ´ng cÃ³ batch má»›i (ká»ƒ tá»« batch Ä‘áº§u tiÃªn).")
try:
    query.awaitTermination()
    print("âœ… Consumer Ä‘Ã£ dá»«ng (do auto-stop hoáº·c signal).")
except Exception as e:
    # Má»™t sá»‘ trÆ°á»ng há»£p Py4J nÃ©m lá»—i khi stop giá»¯a cÃ¡c callback; log rá»“i káº¿t thÃºc Ãªm
    msg = str(e)
    if "Py4J" in msg or "py4j" in msg:
        print(f"âš ï¸ Ignored Py4J termination noise: {e}")
    else:
        print(f"âŒ Streaming job failed: {e}")
        raise

